{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UCI Breast Cancer Dataset - Preprocessing\n",
        "\n",
        "**Project**: Breast Cancer Detection using Machine Learning\n",
        "\n",
        "**Dataset**: UCI Breast Cancer Wisconsin (Diagnostic) Dataset\n",
        "\n",
        "This notebook performs the following preprocessing steps:\n",
        "1. Load the dataset\n",
        "2. Exploratory Data Analysis (EDA)\n",
        "3. Check for missing and inconsistent values\n",
        "4. Encode target labels (M=Malignant, B=Benign)\n",
        "5. Standardize features using StandardScaler\n",
        "6. Split dataset into training and testing sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the Dataset\n",
        "\n",
        "We'll load the UCI Breast Cancer Wisconsin (Diagnostic) dataset from the wdbc.data file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature names based on UCI dataset documentation\n",
        "# 10 base features × 3 (mean, standard error, worst) = 30 features\n",
        "base_features = ['radius', 'texture', 'perimeter', 'area', 'smoothness', \n",
        "                 'compactness', 'concavity', 'concave points', 'symmetry', 'fractal dimension']\n",
        "\n",
        "feature_names = []\n",
        "for stat in ['mean', 'se', 'worst']:\n",
        "    for feature in base_features:\n",
        "        feature_names.append(f\"{feature}_{stat}\")\n",
        "\n",
        "# Define all column names (ID, Diagnosis, 30 features)\n",
        "column_names = ['id', 'diagnosis'] + feature_names\n",
        "\n",
        "# Load the dataset from wdbc.data file\n",
        "df = pd.read_csv('uci_breast_cancer_dataset/wdbc.data', header=None, names=column_names)\n",
        "\n",
        "print(f\"✓ Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {len(df.columns)}\")\n",
        "print(f\"  - ID: 1\")\n",
        "print(f\"  - Diagnosis: 1\")\n",
        "print(f\"  - Features: {len(feature_names)}\")\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display dataset info\n",
        "print(\"Dataset Information:\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "print(\"Statistical Summary:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check target distribution\n",
        "print(\"Target Distribution:\")\n",
        "print(df['diagnosis'].value_counts())\n",
        "print(f\"\\nPercentage:\")\n",
        "print(df['diagnosis'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualize target distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['diagnosis'].value_counts().plot(kind='bar', color=['#ff6b6b', '#4ecdc4'])\n",
        "plt.title('Distribution of Diagnosis', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Diagnosis (M=Malignant, B=Benign)', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Cleaning - Check for Missing and Inconsistent Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values Check:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
        "\n",
        "if missing_values.sum() > 0:\n",
        "    print(\"\\nColumns with missing values:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "else:\n",
        "    print(\"\\n✓ No missing values found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "print(\"Duplicate Rows Check:\")\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(f\"\\nRemoving {duplicates} duplicate rows...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"New shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"✓ No duplicate rows found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for inconsistent values (negative values, outliers)\n",
        "print(\"Checking for negative values (which shouldn't exist in this dataset):\")\n",
        "negative_counts = (df[feature_names] < 0).sum()\n",
        "\n",
        "if negative_counts.sum() > 0:\n",
        "    print(\"\\nColumns with negative values:\")\n",
        "    print(negative_counts[negative_counts > 0])\n",
        "else:\n",
        "    print(\"✓ No negative values found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check data types\n",
        "print(\"Data Types:\")\n",
        "print(df.dtypes.value_counts())\n",
        "print(\"\\n✓ All feature columns are numeric (float64)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix for a subset of features (to keep visualization readable)\n",
        "# Select mean features only\n",
        "mean_features = [col for col in feature_names if 'mean' in col]\n",
        "\n",
        "# Create a numeric version of diagnosis for correlation\n",
        "df['diagnosis_numeric'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = df[mean_features + ['diagnosis_numeric']].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1)\n",
        "plt.title('Correlation Matrix - Mean Features', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Encode Target Labels\n",
        "\n",
        "Convert diagnosis labels from categorical (M/B) to numeric:\n",
        "- M (Malignant) → 1\n",
        "- B (Benign) → 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the diagnosis column\n",
        "# M (Malignant) = 1, B (Benign) = 0\n",
        "df['diagnosis_encoded'] = label_encoder.fit_transform(df['diagnosis'])\n",
        "\n",
        "# Verify encoding\n",
        "print(\"Label Encoding Verification:\")\n",
        "print(df[['diagnosis', 'diagnosis_encoded']].drop_duplicates().sort_values('diagnosis'))\n",
        "print(\"\\n✓ Labels encoded successfully!\")\n",
        "print(\"  M (Malignant) → 1\")\n",
        "print(\"  B (Benign) → 0\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Prepare Features and Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df[feature_names]  # All 30 features\n",
        "y = df['diagnosis_encoded']  # Encoded target (0=Benign, 1=Malignant)\n",
        "\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "print(f\"\\nFeature names ({len(feature_names)} total):\")\n",
        "for i, feature in enumerate(feature_names, 1):\n",
        "    print(f\"{i:2d}. {feature}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Split Dataset into Training and Testing Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y  # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(\"Dataset Split:\")\n",
        "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nClass distribution in testing set:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Standardize Features using StandardScaler\n",
        "\n",
        "Standardization transforms features to have mean=0 and standard deviation=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training data and transform both train and test\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better visualization\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)\n",
        "\n",
        "print(\"✓ Features standardized successfully!\")\n",
        "print(f\"\\nScaled training data shape: {X_train_scaled.shape}\")\n",
        "print(f\"Scaled testing data shape: {X_test_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify standardization (mean ≈ 0, std ≈ 1)\n",
        "print(\"Verification of Standardization (Training Set):\")\n",
        "print(f\"Mean of scaled features: {X_train_scaled.mean(axis=0).mean():.6f} (should be ≈ 0)\")\n",
        "print(f\"Std of scaled features: {X_train_scaled.std(axis=0).mean():.6f} (should be ≈ 1)\")\n",
        "\n",
        "# Show statistics for first few features\n",
        "print(\"\\nStatistics for first 5 features (before and after scaling):\")\n",
        "print(\"\\nBEFORE Scaling:\")\n",
        "print(X_train.iloc[:, :5].describe().loc[['mean', 'std']])\n",
        "print(\"\\nAFTER Scaling:\")\n",
        "print(X_train_scaled_df.iloc[:, :5].describe().loc[['mean', 'std']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualize Feature Distributions (Before vs After Scaling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare distributions before and after scaling for a few features\n",
        "features_to_plot = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "fig.suptitle('Feature Distributions: Before vs After Standardization', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, feature in enumerate(features_to_plot):\n",
        "    # Before scaling\n",
        "    axes[0, idx].hist(X_train[feature], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0, idx].set_title(f'{feature}\\n(Before)', fontsize=10)\n",
        "    axes[0, idx].set_ylabel('Frequency')\n",
        "    axes[0, idx].grid(alpha=0.3)\n",
        "    \n",
        "    # After scaling\n",
        "    axes[1, idx].hist(X_train_scaled_df[feature], bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "    axes[1, idx].set_title(f'{feature}\\n(After)', fontsize=10)\n",
        "    axes[1, idx].set_xlabel('Value')\n",
        "    axes[1, idx].set_ylabel('Frequency')\n",
        "    axes[1, idx].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save preprocessed data to CSV files\n",
        "# Create data directory if it doesn't exist\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "\n",
        "# Save scaled training data\n",
        "X_train_scaled_df.to_csv('data/processed/X_train_scaled.csv', index=False)\n",
        "y_train.to_csv('data/processed/y_train.csv', index=False, header=True)\n",
        "\n",
        "# Save scaled testing data\n",
        "X_test_scaled_df.to_csv('data/processed/X_test_scaled.csv', index=False)\n",
        "y_test.to_csv('data/processed/y_test.csv', index=False, header=True)\n",
        "\n",
        "# Save the scaler for future use\n",
        "joblib.dump(scaler, 'data/processed/scaler.pkl')\n",
        "\n",
        "# Also save feature names for reference\n",
        "with open('data/processed/feature_names.txt', 'w') as f:\n",
        "    for feature in feature_names:\n",
        "        f.write(f\"{feature}\\n\")\n",
        "\n",
        "print(\"✓ Preprocessed data saved successfully!\")\n",
        "print(\"\\nSaved files:\")\n",
        "print(\"  - data/processed/X_train_scaled.csv\")\n",
        "print(\"  - data/processed/y_train.csv\")\n",
        "print(\"  - data/processed/X_test_scaled.csv\")\n",
        "print(\"  - data/processed/y_test.csv\")\n",
        "print(\"  - data/processed/scaler.pkl\")\n",
        "print(\"  - data/processed/feature_names.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary of Preprocessing Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n1. Dataset Loaded:\")\n",
        "print(f\"   - Source: uci_breast_cancer_dataset/wdbc.data\")\n",
        "print(f\"   - Total samples: {len(df)}\")\n",
        "print(f\"   - Total features: {len(feature_names)}\")\n",
        "print(f\"   - Malignant cases: {(df['diagnosis'] == 'M').sum()}\")\n",
        "print(f\"   - Benign cases: {(df['diagnosis'] == 'B').sum()}\")\n",
        "\n",
        "print(f\"\\n2. Data Cleaning:\")\n",
        "print(f\"   - Missing values: 0\")\n",
        "print(f\"   - Duplicate rows: 0\")\n",
        "print(f\"   - Negative values: 0\")\n",
        "\n",
        "print(f\"\\n3. Label Encoding:\")\n",
        "print(f\"   - M (Malignant) → 1\")\n",
        "print(f\"   - B (Benign) → 0\")\n",
        "\n",
        "print(f\"\\n4. Train-Test Split:\")\n",
        "print(f\"   - Training samples: {len(X_train)} (80%)\")\n",
        "print(f\"   - Testing samples: {len(X_test)} (20%)\")\n",
        "\n",
        "print(f\"\\n5. Feature Standardization:\")\n",
        "print(f\"   - Method: StandardScaler\")\n",
        "print(f\"   - Mean ≈ 0, Std ≈ 1\")\n",
        "\n",
        "print(f\"\\n6. Data Saved:\")\n",
        "print(f\"   - Location: data/processed/\")\n",
        "print(f\"   - Files: CSV format + scaler pickle\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ PREPROCESSING COMPLETE - Ready for Model Training!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
