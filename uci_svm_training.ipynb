{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7c39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# UCI Breast Cancer Dataset - SVM Model Training\n",
    "\n",
    "## Table of Contents\n",
    "1. Load Preprocessed Data\n",
    "2. Train SVM Model\n",
    "3. Evaluate Performance\n",
    "4. Confusion Matrix & Classification Report\n",
    "5. ROC Curve Analysis\n",
    "6. Hyperparameter Tuning with Grid Search\n",
    "7. Save Best Model\n",
    "8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d17ced",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 1. Import  Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ef93c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "# SVM and model selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7d296",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 2. Loading previously preprocessed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdade7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "X_train = pd.read_csv('data/processed/X_train_scaled.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "X_test = pd.read_csv('data/processed/X_test_scaled.csv')\n",
    "y_test = pd.read_csv('data/processed/y_test.csv')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "print(f\"âœ“ Data loaded successfully!\")\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"  Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"  Training - Benign: {np.sum(y_train == 0)}, Malignant: {np.sum(y_train == 1)}\")\n",
    "print(f\"  Testing - Benign: {np.sum(y_test == 0)}, Malignant: {np.sum(y_test == 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31229a69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 3. Train Basic SVM Model\n",
    "basic svm with rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e67672",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create and train SVM model\n",
    "print(\"Training SVM model with RBF kernel...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Initialize SVM with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', random_state=42, probability=True)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time() - start_time\n",
    "\n",
    "print(f\"Model trained successfully in {training_time:.2f} seconds!\")\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Kernel: {svm_model.kernel}\")\n",
    "print(f\"  C (regularization): {svm_model.C}\")\n",
    "print(f\"  Gamma: {svm_model.gamma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0b179",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 4. Make Predictions and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93bd1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "y_pred_proba = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVM MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f} ({roc_auc*100:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"\\n5-Fold Cross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158ccdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 5. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ad039",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Benign (0)', 'Malignant (1)'],\n",
    "            yticklabels=['Benign (0)', 'Malignant (1)'],\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "plt.title('Confusion Matrix - SVM Model', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "\n",
    "# Add text annotations\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "plt.text(0.5, -0.15, f'True Negatives: {tn}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "plt.text(0.5, -0.20, f'False Positives: {fp}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "plt.text(0.5, -0.25, f'False Negatives: {fn}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "plt.text(0.5, -0.30, f'True Positives: {tp}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (Correct Benign):    {tn}\")\n",
    "print(f\"  False Positives (Benign as Malignant): {fp}\")\n",
    "print(f\"  False Negatives (Malignant as Benign): {fn}\")\n",
    "print(f\"  True Positives (Correct Malignant):  {tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdec9af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 6. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0f406",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['Benign (0)', 'Malignant (1)'],\n",
    "                          digits=4))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f1ea1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 7. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac01d1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='#e74c3c', linewidth=3, label=f'SVM (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curve - SVM Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"  This means the model has a {roc_auc*100:.2f}% chance of correctly distinguishing\")\n",
    "print(f\"  between benign and malignant cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da63a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 8. Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e229b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING WITH GRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "#defining parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "print(\"\\nSearching for best hyperparameters...\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "print(f\"This may take a few minutes...\\n\")\n",
    "\n",
    "#performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42, probability=True),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "search_time = time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Grid search completed in {search_time:.2f} seconds!\")\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation F1-score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc83732",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 9. Evaluating Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b7aca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#getting best model from grid search\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "#making predictions with the optimized model\n",
    "y_pred_optimized = best_svm_model.predict(X_test)\n",
    "y_pred_proba_optimized = best_svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#calculating metrics for optimized model\n",
    "accuracy_opt = accuracy_score(y_test, y_pred_optimized)\n",
    "precision_opt = precision_score(y_test, y_pred_optimized)\n",
    "recall_opt = recall_score(y_test, y_pred_optimized)\n",
    "f1_opt = f1_score(y_test, y_pred_optimized)\n",
    "roc_auc_opt = roc_auc_score(y_test, y_pred_proba_optimized)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZED SVM MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:  {accuracy_opt:.4f} ({accuracy_opt*100:.2f}%)\")\n",
    "print(f\"Precision: {precision_opt:.4f} ({precision_opt*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall_opt:.4f} ({recall_opt*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {f1_opt:.4f} ({f1_opt*100:.2f}%)\")\n",
    "print(f\"ROC-AUC:   {roc_auc_opt:.4f} ({roc_auc_opt*100:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "#comparing with the basic model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Basic SVM': [accuracy, precision, recall, f1, roc_auc],\n",
    "    'Optimized SVM': [accuracy_opt, precision_opt, recall_opt, f1_opt, roc_auc_opt],\n",
    "    'Improvement': [\n",
    "        accuracy_opt - accuracy,\n",
    "        precision_opt - precision,\n",
    "        recall_opt - recall,\n",
    "        f1_opt - f1,\n",
    "        roc_auc_opt - roc_auc\n",
    "    ]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b0437",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 10. Visualizing Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93718ef8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#creating comparison visualization\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "basic_scores = [accuracy, precision, recall, f1, roc_auc]\n",
    "optimized_scores = [accuracy_opt, precision_opt, recall_opt, f1_opt, roc_auc_opt]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, basic_scores, width, label='Basic SVM', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, optimized_scores, width, label='Optimized SVM', color='#2ecc71')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Basic vs Optimized SVM Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0.9, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "#adding value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
